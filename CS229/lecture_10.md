[<< Lecture 9](lecture_9.md) â€¢ [Lecture 11 >>](lecture_11.md)
## Lecture 10 - Decision Trees and Ensemble Methods

[![Lecture 10 - Decision Trees and Ensemble Methods | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dwr9gUr-eWdA%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D10)](https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10)

### Overview

### Timestamps
  
[0:27](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=27): ğŸ“ Raphael Townshend gives a lecture on decision trees and ensemble methods.  
[6:00](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=360): ğŸ“š The video discusses the concept of splitting regions and defining a split function in machine learning.  
[15:06](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=906): ğŸ“Š The video discusses the limitations of misclassification loss and introduces cross-entropy loss as a more sensitive alternative.  
[21:00](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=1260): ğŸ“Š The video discusses the concept of misclassification loss and cross-entropy loss in binary classification problems and provides an intuitive explanation for their differences.  
[27:23](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=1643): ğŸ“‰ The video discusses the shape of loss curves in machine learning models.  
[35:29](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=2129): ğŸŒ The video discusses the concept of splitting data based on categories and subsets.  
[43:57](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=2637): ğŸŒ³ The video explains the runtime and training process of decision trees.  
[51:19](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=3079): ğŸŒ³ Ensembling can improve the performance of decision trees and is commonly used in Kaggle competitions.  
[1:00:59](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=3659): ğŸ’ Bagging, or bootstrap aggregation, is a method used in statistics to measure the uncertainty of an estimate by drawing multiple training sets from a population.  
[1:07:48](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=4068): âš™ï¸ Bootstrapping can decrease variance but slightly increase bias in models.  
[1:15:09](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=4509): ğŸ’¼ Boosting is a technique that decreases the bias of models by adding predictions from previous models into the ensemble.

Timestamps by Tammy AI

[<< Lecture 9](lecture_9.md) â€¢ [Lecture 11 >>](lecture_11.md)